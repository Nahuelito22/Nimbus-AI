{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc387b53-c5ed-4027-b6d8-315bf18576b5",
   "metadata": {},
   "source": [
    "# Modelo Multimodal V3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc8483-6fe3-4afd-9e97-1d5726e3b98d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importación de librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ff575-10f5-45c2-ae1b-bae8fce6dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55dfddfe-95c7-41e2-a1e4-f10bd9040faa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m \n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxr\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m \n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyproj\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Proj\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np \n",
    "import xarray as xr\n",
    "import cv2 \n",
    "from tqdm import tqdm\n",
    "from pyproj import Proj\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6117f-4344-489b-ae63-af4300d5c86f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importación de datos y fusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc624c-046a-41df-9a90-f6d7d203f358",
   "metadata": {},
   "source": [
    "En esta notebook se importarán los datos tabulares junto con las imágenes satelitales. \n",
    "Posteriormente, se procederá a la fusión de ambos datasets con el fin de obtener un único conjunto integrado, denominado df_multimodal, el cual será utilizado para el entrenamiento de la red neuronal convolucional (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ded073-ef3b-4e38-9e58-63979fab4ce6",
   "metadata": {},
   "source": [
    "En el siguiente fragmento de código se realizan tres pasos principales:\n",
    "\n",
    "1) Carga de datasets\n",
    "   * dataset_final_enriquecido.csv: contiene los datos tabulares.\n",
    "   * imagenes_satelitales: conjunto de imágenes satelitales descargadas previamente.\n",
    "\n",
    "2) Identificación de carpetas e imágenes\n",
    "Cada carpeta es reconocida por sus 5 imágenes correspondientes, a partir de las cuales se extrae la fecha asociada al evento.\n",
    "\n",
    "3) Integración de datos\n",
    "Una vez obtenida la fecha de cada imagen, se agrupan y combinan la información proveniente de ambos datasets, generando un único dataset integrado que incluye tanto datos tabulares como imágenes satelitales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c267d52b-bc34-4cfa-9cce-c57bd6df9e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando la preparación de datos para el Modelo Multimodal V3.0 ---\n",
      "✅ Dataset tabular completo cargado con 37399 filas.\n",
      "✅ Se encontraron 101 fechas con secuencias de imágenes completas.\n",
      "\n",
      "✅ Dataset sincronizado. Ahora tenemos 466 filas listas para el modelo.\n",
      "\n",
      "--- Muestra del Dataset para el Modelo V3.0 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station_name</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>granizo</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>om_rain_sum</th>\n",
       "      <th>om_snowfall_sum</th>\n",
       "      <th>om_precipitation_hours</th>\n",
       "      <th>om_wind_gusts_10m_max</th>\n",
       "      <th>om_wind_direction_10m_dominant</th>\n",
       "      <th>om_shortwave_radiation_sum</th>\n",
       "      <th>om_et0_fao_evapotranspiration</th>\n",
       "      <th>om_dew_point_2m_mean</th>\n",
       "      <th>om_relative_humidity_2m_mean</th>\n",
       "      <th>om_pressure_msl_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24283</th>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>MALARGUE, AR</td>\n",
       "      <td>6.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>-35.48</td>\n",
       "      <td>-69.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>105.97220</td>\n",
       "      <td>19.76</td>\n",
       "      <td>4.493612</td>\n",
       "      <td>5.172083</td>\n",
       "      <td>48.640656</td>\n",
       "      <td>1009.84160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24284</th>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>MENDOZA AERO, AR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.2</td>\n",
       "      <td>31.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-32.83</td>\n",
       "      <td>-68.78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.160000</td>\n",
       "      <td>173.85895</td>\n",
       "      <td>20.04</td>\n",
       "      <td>4.896718</td>\n",
       "      <td>11.737751</td>\n",
       "      <td>49.119923</td>\n",
       "      <td>1007.72090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24285</th>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>SAN MARTIN, AR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.3</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0</td>\n",
       "      <td>-33.08</td>\n",
       "      <td>-68.48</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38.519997</td>\n",
       "      <td>137.53076</td>\n",
       "      <td>20.21</td>\n",
       "      <td>4.709652</td>\n",
       "      <td>13.338666</td>\n",
       "      <td>55.882126</td>\n",
       "      <td>1008.12080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24286</th>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>SAN RAFAEL, AR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.4</td>\n",
       "      <td>33.5</td>\n",
       "      <td>13.8</td>\n",
       "      <td>1</td>\n",
       "      <td>-34.58</td>\n",
       "      <td>-68.33</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>39.960000</td>\n",
       "      <td>357.63052</td>\n",
       "      <td>17.44</td>\n",
       "      <td>4.145039</td>\n",
       "      <td>12.633335</td>\n",
       "      <td>57.392117</td>\n",
       "      <td>1008.84590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24287</th>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>MALARGUE, AR</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.4</td>\n",
       "      <td>25.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-35.48</td>\n",
       "      <td>-69.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.960000</td>\n",
       "      <td>86.02075</td>\n",
       "      <td>21.12</td>\n",
       "      <td>4.559690</td>\n",
       "      <td>4.172083</td>\n",
       "      <td>48.957110</td>\n",
       "      <td>1010.71674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      station_name  PRCP  SNWD  TAVG  TMAX  TMIN  granizo  \\\n",
       "24283 2017-03-25      MALARGUE, AR   6.1   NaN  16.1  26.0   9.8        0   \n",
       "24284 2017-03-25  MENDOZA AERO, AR   0.0   NaN  23.2  31.3   NaN        0   \n",
       "24285 2017-03-25    SAN MARTIN, AR   NaN   NaN  23.3  33.0  16.4        0   \n",
       "24286 2017-03-25    SAN RAFAEL, AR   NaN   NaN  20.4  33.5  13.8        1   \n",
       "24287 2017-03-26      MALARGUE, AR   2.0   NaN  16.4  25.2   9.5        0   \n",
       "\n",
       "       latitude  longitude  ...  om_rain_sum  om_snowfall_sum  \\\n",
       "24283    -35.48     -69.58  ...          0.6              0.0   \n",
       "24284    -32.83     -68.78  ...          0.0              0.0   \n",
       "24285    -33.08     -68.48  ...          2.1              0.0   \n",
       "24286    -34.58     -68.33  ...          2.6              0.0   \n",
       "24287    -35.48     -69.58  ...          0.5              0.0   \n",
       "\n",
       "       om_precipitation_hours  om_wind_gusts_10m_max  \\\n",
       "24283                     1.0              54.000000   \n",
       "24284                     0.0              38.160000   \n",
       "24285                     5.0              38.519997   \n",
       "24286                     4.0              39.960000   \n",
       "24287                     1.0              48.960000   \n",
       "\n",
       "       om_wind_direction_10m_dominant  om_shortwave_radiation_sum  \\\n",
       "24283                       105.97220                       19.76   \n",
       "24284                       173.85895                       20.04   \n",
       "24285                       137.53076                       20.21   \n",
       "24286                       357.63052                       17.44   \n",
       "24287                        86.02075                       21.12   \n",
       "\n",
       "       om_et0_fao_evapotranspiration  om_dew_point_2m_mean  \\\n",
       "24283                       4.493612              5.172083   \n",
       "24284                       4.896718             11.737751   \n",
       "24285                       4.709652             13.338666   \n",
       "24286                       4.145039             12.633335   \n",
       "24287                       4.559690              4.172083   \n",
       "\n",
       "       om_relative_humidity_2m_mean  om_pressure_msl_mean  \n",
       "24283                     48.640656            1009.84160  \n",
       "24284                     49.119923            1007.72090  \n",
       "24285                     55.882126            1008.12080  \n",
       "24286                     57.392117            1008.84590  \n",
       "24287                     48.957110            1010.71674  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conteo de eventos en el nuevo dataset ---\n",
      "granizo\n",
      "0    417\n",
      "1     49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Iniciando la preparación de datos para el Modelo Multimodal V3.0 ---\")\n",
    "\n",
    "# --- 1. Cargar nuestro dataset tabular completo ---\n",
    "ruta_dataset = \"../data/processed/dataset_final_enriquecido.csv\"\n",
    "try:\n",
    "    df_completo = pd.read_csv(ruta_dataset, parse_dates=['date'])\n",
    "    print(f\"✅ Dataset tabular completo cargado con {len(df_completo)} filas.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: No se encontró el archivo '{ruta_dataset}'. Asegúrate de que exista.\")\n",
    "    df_completo = None\n",
    "\n",
    "if df_completo is not None:\n",
    "    # --- 2. Identificar las secuencias de imágenes que SÍ tenemos ---\n",
    "    ruta_imagenes = \"../data/raw/imagenes_satelitales/\"\n",
    "    \n",
    "    # Escaneamos la carpeta de imágenes para obtener la lista de fechas que se descargaron.\n",
    "    # Nos aseguramos de que cada carpeta tenga sus 5 imágenes.\n",
    "    carpetas_completas = []\n",
    "    if os.path.exists(ruta_imagenes):\n",
    "        for carpeta in os.listdir(ruta_imagenes):\n",
    "            ruta_carpeta = os.path.join(ruta_imagenes, carpeta)\n",
    "            if os.path.isdir(ruta_carpeta) and len(os.listdir(ruta_carpeta)) >= 5:\n",
    "                carpetas_completas.append(carpeta)\n",
    "    \n",
    "    # Extraemos solo la fecha (YYYY-MM-DD) del nombre de cada carpeta completa\n",
    "    fechas_con_imagenes = [re.search(r\"\\d{4}-\\d{2}-\\d{2}\", carpeta).group(0) for carpeta in carpetas_completas if re.search(r\"\\d{4}-\\d{2}-\\d{2}\", carpeta)]\n",
    "    fechas_con_imagenes = pd.to_datetime(fechas_con_imagenes)\n",
    "\n",
    "    print(f\"✅ Se encontraron {len(fechas_con_imagenes)} fechas con secuencias de imágenes completas.\")\n",
    "\n",
    "    # --- 3. Sincronizar: Filtrar el dataset tabular ---\n",
    "    # Creamos nuestro DataFrame final, quedándonos solo con las filas de los días\n",
    "    # para los cuales tenemos una secuencia de imágenes completa.\n",
    "    df_multimodal = df_completo[df_completo['date'].isin(fechas_con_imagenes)].copy()\n",
    "\n",
    "    print(f\"\\n✅ Dataset sincronizado. Ahora tenemos {len(df_multimodal)} filas listas para el modelo.\")\n",
    "\n",
    "    # --- 4. Verificación ---\n",
    "    print(\"\\n--- Muestra del Dataset para el Modelo V3.0 ---\")\n",
    "    display(df_multimodal.head())\n",
    "    print(\"\\n--- Conteo de eventos en el nuevo dataset ---\")\n",
    "    print(df_multimodal['granizo'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f50a322-806b-464b-85d0-1740159e418a",
   "metadata": {},
   "source": [
    "Como se puede observar, los datos fueron cargados correctamente.\n",
    "En cuanto a la diferencia entre clases, esta se debe a que existen fechas en las que en una estación se registró granizo y en otra no.\n",
    "\n",
    "Lejos de ser un problema, esta situación puede aportar información valiosa al modelo, ya que le permite identificar el evento como específico de una localización determinada, contribuyendo así a una suerte de “geolocalización” del fenómeno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983919b-6fbc-467d-800a-589d7eb1233e",
   "metadata": {},
   "source": [
    "Comprobación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d824c8-99c2-4bf1-bdc0-b87dc5b5220b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                                0\n",
       "station_name                        0\n",
       "PRCP                              339\n",
       "SNWD                              464\n",
       "TAVG                                0\n",
       "TMAX                              244\n",
       "TMIN                               46\n",
       "granizo                             0\n",
       "latitude                            5\n",
       "longitude                           5\n",
       "om_weather_code                     5\n",
       "om_temperature_2m_max               5\n",
       "om_temperature_2m_min               5\n",
       "om_apparent_temperature_mean        5\n",
       "om_precipitation_sum                5\n",
       "om_rain_sum                         5\n",
       "om_snowfall_sum                     5\n",
       "om_precipitation_hours              5\n",
       "om_wind_gusts_10m_max               5\n",
       "om_wind_direction_10m_dominant      5\n",
       "om_shortwave_radiation_sum          5\n",
       "om_et0_fao_evapotranspiration       5\n",
       "om_dew_point_2m_mean                5\n",
       "om_relative_humidity_2m_mean        5\n",
       "om_pressure_msl_mean                5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_multimodal.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb594370-efa0-44e4-9054-eecf2f8470cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Estrategia de imputación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94a34c-429d-42df-9ed9-fde5f258f07b",
   "metadata": {},
   "source": [
    "Como se observó previamente, ocurre la misma situación que en el dataset obtenido a partir de NOAA.\n",
    "\n",
    "Para resolver este inconveniente, se aplicará la misma estrategia utilizada anteriormente: imputación mediante el promedio entre los valores disponibles y los obtenidos a través de la API de Open-Meteo.\n",
    "En los casos en los que no se disponga de datos adicionales, se utilizarán directamente los valores proporcionados por Open-Meteo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b114e-c68e-464e-ba48-77012678bed8",
   "metadata": {},
   "source": [
    "El siguiente script implementa tres estrategias de imputación de datos:\n",
    "\n",
    "1) Relleno con datos de Open-Meteo:\n",
    "Los valores faltantes en el dataset de NOAA se completan utilizando la información obtenida desde la API de Open-Meteo.\n",
    "\n",
    "2) Interpolación temporal:\n",
    "Se aplica interpolación en función del tiempo para estimar valores intermedios en las series temporales.\n",
    "\n",
    "3) Relleno residual con ceros:\n",
    "En caso de que aún persistan valores faltantes, estos se reemplazan por 0, a fin de evitar la corrupción de la estructura temporal del dataset climatológico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64aa2fc9-eb95-4951-81d8-29490eb1b2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando limpieza final de NaN...\n",
      "  -> Huecos en 'TMAX' rellenados con datos de 'om_temperature_2m_max'.\n",
      "  -> Huecos en 'TMIN' rellenados con datos de 'om_temperature_2m_min'.\n",
      "  -> Huecos en 'TAVG' rellenados con datos de 'om_apparent_temperature_mean'.\n",
      "  -> Huecos en 'PRCP' rellenados con datos de 'om_precipitation_sum'.\n",
      "  -> Huecos restantes rellenados con interpolación.\n",
      "\n",
      "✅ Limpieza final completada.\n",
      "\n",
      "--- Información del Dataset 100% Limpio ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 466 entries, 0 to 465\n",
      "Data columns (total 25 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   date                            466 non-null    datetime64[ns]\n",
      " 1   station_name                    466 non-null    object        \n",
      " 2   PRCP                            466 non-null    float64       \n",
      " 3   SNWD                            466 non-null    float64       \n",
      " 4   TAVG                            466 non-null    float64       \n",
      " 5   TMAX                            466 non-null    float64       \n",
      " 6   TMIN                            466 non-null    float64       \n",
      " 7   granizo                         466 non-null    int64         \n",
      " 8   latitude                        466 non-null    float64       \n",
      " 9   longitude                       466 non-null    float64       \n",
      " 10  om_weather_code                 466 non-null    float64       \n",
      " 11  om_temperature_2m_max           466 non-null    float64       \n",
      " 12  om_temperature_2m_min           466 non-null    float64       \n",
      " 13  om_apparent_temperature_mean    466 non-null    float64       \n",
      " 14  om_precipitation_sum            466 non-null    float64       \n",
      " 15  om_rain_sum                     466 non-null    float64       \n",
      " 16  om_snowfall_sum                 466 non-null    float64       \n",
      " 17  om_precipitation_hours          466 non-null    float64       \n",
      " 18  om_wind_gusts_10m_max           466 non-null    float64       \n",
      " 19  om_wind_direction_10m_dominant  466 non-null    float64       \n",
      " 20  om_shortwave_radiation_sum      466 non-null    float64       \n",
      " 21  om_et0_fao_evapotranspiration   466 non-null    float64       \n",
      " 22  om_dew_point_2m_mean            466 non-null    float64       \n",
      " 23  om_relative_humidity_2m_mean    466 non-null    float64       \n",
      " 24  om_pressure_msl_mean            466 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(22), int64(1), object(1)\n",
      "memory usage: 91.1+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matia\\AppData\\Local\\Temp\\ipykernel_10400\\3017496548.py:21: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_multimodal = df_multimodal.interpolate(method='time')\n"
     ]
    }
   ],
   "source": [
    "# df_multimodal\n",
    "print(\"Iniciando limpieza final de NaN...\")\n",
    "\n",
    "# Estrategia 1\n",
    "mapeo_columnas = {\n",
    "    'TMAX': 'om_temperature_2m_max',\n",
    "    'TMIN': 'om_temperature_2m_min',\n",
    "    'TAVG': 'om_apparent_temperature_mean',\n",
    "    'PRCP': 'om_precipitation_sum'\n",
    "}\n",
    "\n",
    "for col_noaa, col_om in mapeo_columnas.items():\n",
    "    if col_noaa in df_multimodal.columns and col_om in df_multimodal.columns:\n",
    "        df_multimodal[col_noaa] = df_multimodal[col_noaa].fillna(df_multimodal[col_om])\n",
    "        print(f\"  -> Huecos en '{col_noaa}' rellenados con datos de '{col_om}'.\")\n",
    "\n",
    "# Estrategia 2:\n",
    "# Primero establecemos la fecha como índice.\n",
    "df_multimodal = df_multimodal.set_index('date')\n",
    "# Interpolamos.\n",
    "df_multimodal = df_multimodal.interpolate(method='time')\n",
    "# Reseteamos el índice para que 'date' vuelva a ser una columna.\n",
    "df_multimodal = df_multimodal.reset_index()\n",
    "print(\"  -> Huecos restantes rellenados con interpolación.\")\n",
    "\n",
    "# Estrategia 3: \n",
    "df_multimodal = df_multimodal.fillna(0)\n",
    "\n",
    "print(\"\\n✅ Limpieza final completada.\")\n",
    "\n",
    "# Verificamos\n",
    "print(\"\\n--- Información del Dataset 100% Limpio ---\")\n",
    "df_multimodal.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1efabe-580d-49f8-88ad-ed4d272fa6bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preparación para el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56422894-4625-4765-b36d-4bb5e846c896",
   "metadata": {},
   "source": [
    "Este código se encargará de:\n",
    "1) Seleccionar las características (X) y el objetivo (y).\n",
    "2) Dividir los datos en conjuntos de entrenamiento y prueba.\n",
    "3) Escalar las características para optimizar el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c0c3b-45d8-4030-81d4-c0141812d47e",
   "metadata": {},
   "source": [
    "1) Las Características de Tiempo Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8beeebb-d432-4a74-807f-fdde1a2ef2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando nuevas características...\n",
      "✅ Nuevas características ('año', 'mes', etc.) creadas exitosamente.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creando nuevas características...\")\n",
    "df_multimodal['año'] = df_multimodal['date'].dt.year\n",
    "df_multimodal['mes'] = df_multimodal['date'].dt.month\n",
    "df_multimodal['dia_del_año'] = df_multimodal['date'].dt.dayofyear\n",
    "df_multimodal['rango_temp_diario'] = df_multimodal['TMAX'] - df_multimodal['TMIN']\n",
    "print(\"✅ Nuevas características ('año', 'mes', etc.) creadas exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40aa63-1d08-41f9-9ce8-ccbe8420c7ec",
   "metadata": {},
   "source": [
    "2) Los datos tabulares (X_tabular) y el objetivo (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da54238f-03b6-4205-b3b5-bfec926bcf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos tabulares separados. Usaremos 19 características numéricas.\n"
     ]
    }
   ],
   "source": [
    "y = df_multimodal['granizo']\n",
    "\n",
    "# Seleccionamos todas las columnas numéricas como características,\n",
    "# excluyendo las que no son predictivas o ya están representadas.\n",
    "columnas_a_excluir = [\n",
    "    'date', 'station_name', 'granizo', 'latitude', 'longitude', 'año',\n",
    "    # Excluimos las 'om_' que ya usamos para rellenar y evitar redundancia\n",
    "    'om_temperature_2m_max', 'om_temperature_2m_min', \n",
    "    'om_apparent_temperature_mean', 'om_precipitation_sum'\n",
    "]\n",
    "# Seleccionamos todas las columnas que no están en la lista de exclusión\n",
    "X_tabular = df_multimodal.drop(columns=columnas_a_excluir)\n",
    "\n",
    "print(f\"✅ Datos tabulares separados. Usaremos {X_tabular.shape[1]} características numéricas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109900a-2384-4225-8ad7-682e3695699f",
   "metadata": {},
   "source": [
    "3) División de Entrenamiento y Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "054be84c-1dd0-4b1e-aa23-7fec7f3bbe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Datos divididos en conjuntos de entrenamiento y prueba.\n"
     ]
    }
   ],
   "source": [
    "# Necesitamos dividir los índices para poder seleccionar las imágenes correctas después\n",
    "indices = df_multimodal.index\n",
    "train_indices, test_indices, y_train, y_test = train_test_split(\n",
    "    indices, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Ahora seleccionamos los datos tabulares usando los índices\n",
    "X_tabular_train = X_tabular.loc[train_indices]\n",
    "X_tabular_test = X_tabular.loc[test_indices]\n",
    "\n",
    "print(\"\\n✅ Datos divididos en conjuntos de entrenamiento y prueba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6badda81-8136-4232-8cc0-c2ba8e710251",
   "metadata": {},
   "source": [
    "4) Escalado de Características Tabulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efb44354-6045-4248-ba4e-e63934e2597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos tabulares escalados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_tabular_train_scaled = scaler.fit_transform(X_tabular_train)\n",
    "X_tabular_test_scaled = scaler.transform(X_tabular_test)\n",
    "print(\"✅ Datos tabulares escalados exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28248bf1-d50a-4641-b98c-0f8b0d49e966",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  Procesar y Dividir el Dataset de Imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a80d6-ef2c-440e-ae72-06be4af3873c",
   "metadata": {},
   "source": [
    "1) Preparación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ef1f2-8104-4cc4-a980-b16dd248b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "ruta_base_imagenes = \"../data/raw/imagenes_satelitales/\"\n",
    "IMG_SIZE = 64 # Tamaño estándar para las imágenes\n",
    "\n",
    "# Diccionario para guardar en caché los objetos de proyección y evitar recalcularlos\n",
    "proj_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ccdfd-20e1-4e3e-a17f-40190986e82c",
   "metadata": {},
   "source": [
    "2) Función para procesar UNA secuencia de 5 imágenes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf00c5-23c2-4e8f-8377-d483cdc578dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "def procesar_secuencia(fecha, granizo_status):\n",
    "    nombre_carpeta = f\"{fecha.strftime('%Y-%m-%d')}_{('granizo' if granizo_status == 1 else 'no_granizo')}\"\n",
    "    ruta_carpeta = os.path.join(ruta_base_imagenes, nombre_carpeta)\n",
    "    \n",
    "    if not os.path.exists(ruta_carpeta):\n",
    "        return None # Si la carpeta no existe, devolvemos Nada\n",
    "        \n",
    "    archivos_secuencia = sorted(os.listdir(ruta_carpeta))\n",
    "    secuencia_procesada = []\n",
    "    \n",
    "    for nombre_archivo in archivos_secuencia[:5]: # Nos aseguramos de tomar solo 5\n",
    "        ruta_completa = os.path.join(ruta_carpeta, nombre_archivo)\n",
    "        ds = xr.open_dataset(ruta_completa)\n",
    "        \n",
    "        # Recorte geoespacial\n",
    "        proj_info = ds.goes_imager_projection\n",
    "        h_sat = proj_info.perspective_point_height\n",
    "        lon_cen = proj_info.longitude_of_projection_origin\n",
    "        \n",
    "        # Usamos caché para el objeto Proj\n",
    "        if lon_cen not in proj_cache:\n",
    "            proj_cache[lon_cen] = Proj(proj='geos', h=h_sat, lon_0=lon_cen)\n",
    "        p = proj_cache[lon_cen]\n",
    "        \n",
    "        x1, y1 = p(-70.5, -37.5); x2, y2 = p(-66.5, -32.0)\n",
    "        ds['x'] = ds['x'] * h_sat; ds['y'] = ds['y'] * h_sat\n",
    "        recorte = ds.sel(x=slice(x1, x2), y=slice(y2, y1))['CMI'].values\n",
    "        \n",
    "        # Redimensionar y Normalizar\n",
    "        recorte_redimensionado = cv2.resize(recorte, (IMG_SIZE, IMG_SIZE))\n",
    "        recorte_normalizado = (recorte_redimensionado - np.nanmin(recorte_redimensionado)) / (np.nanmax(recorte_redimensionado) - np.nanmin(recorte_redimensionado) + 1e-6)\n",
    "        secuencia_procesada.append(np.nan_to_num(recorte_normalizado))\n",
    "        \n",
    "    # Apilamos las 5 imágenes para formar un array de (64, 64, 5)\n",
    "    return np.stack(secuencia_procesada, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31384b00-2c0f-48f2-a588-bbb866a79548",
   "metadata": {},
   "source": [
    "3) Procesar TODAS las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cb48c-370b-4015-acf4-3e48969109d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "print(\"Iniciando procesamiento de todas las secuencias de imágenes... (esto puede tardar)\")\n",
    "X_images_full = [procesar_secuencia(row['date'], row['granizo']) for index, row in tqdm(df_multimodal.iterrows(), total=len(df_multimodal))]\n",
    "\n",
    "# Convertimos la lista de imágenes a un único array de NumPy\n",
    "X_images_full = np.array(X_images_full)\n",
    "print(f\"\\n✅ Procesamiento de imágenes completo. Dimensiones: {X_images_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4becef83-f3b7-4642-aaac-e16898e5a2c1",
   "metadata": {},
   "source": [
    "4) Dividir el dataset de imágenes usando los mismos índices que los datos tabulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3700c3-dceb-4881-92c7-7902b87452cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images_train = X_images_full[train_indices]\n",
    "X_images_test = X_images_full[test_indices]\n",
    "print(\"\\n✅ Dataset de imágenes dividido en conjuntos de entrenamiento y prueba.\")\n",
    "print(f\"Dimensiones de X_images_train: {X_images_train.shape}\")\n",
    "print(f\"Dimensiones de X_images_test: {X_images_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2578a45-2e0e-4f5a-8deb-ebb89dc0adba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Construir la Arquitectura Multimodal (V3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec161d-e685-4b13-8fef-51b762131ed4",
   "metadata": {},
   "source": [
    "1) Definir la Rama para los Datos Tabulares (MLP)\n",
    "2) Definir la Rama para las Imágenes (CNN)\n",
    "3) Unir las dos Ramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42468c-46a8-44df-badf-b7a0f4ee4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "# Esta parte del cerebro se especializa en leer los números.\n",
    "input_tabular = keras.Input(shape=(X_tabular_train_scaled.shape[1],), name=\"input_numerico\")\n",
    "x = layers.Dense(32, activation=\"relu\")(input_tabular)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "# Guardamos la salida de esta rama en una variable\n",
    "rama_tabular_salida = layers.Dense(8, activation=\"relu\")(x)\n",
    "\n",
    "\n",
    "#2\n",
    "# Esta parte del cerebro se especializa en \"ver\" las imágenes.\n",
    "input_imagenes = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 5), name=\"input_visual\") # 64x64 píxeles, 5 imágenes en secuencia\n",
    "y = layers.Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\")(input_imagenes)\n",
    "y = layers.MaxPooling2D(pool_size=(2, 2))(y)\n",
    "y = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(y)\n",
    "y = layers.MaxPooling2D(pool_size=(2, 2))(y)\n",
    "y = layers.Flatten()(y) # Aplanamos el resultado para poder unirlo\n",
    "# Guardamos la salida de esta rama en una variable\n",
    "rama_imagenes_salida = layers.Dense(16, activation=\"relu\")(y)\n",
    "\n",
    "\n",
    "#3\n",
    "# Concatenamos las salidas de ambos \"expertos\"\n",
    "ramas_unidas = layers.concatenate([rama_tabular_salida, rama_imagenes_salida])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90368e83-0829-4005-8af1-244c65061d5a",
   "metadata": {},
   "source": [
    "4) Cabeza de Clasificación Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e030d0-5abd-4b50-bceb-f509a1ab5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un cerebro final que toma la decisión basándose en la información combinada\n",
    "z = layers.Dense(16, activation=\"relu\")(ramas_unidas)\n",
    "z = layers.Dropout(0.5)(z) # Usamos Dropout para regularizar\n",
    "output_final = layers.Dense(1, activation=\"sigmoid\", name=\"output_final\")(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b594b0e-7f35-48df-8b5c-8986a9093126",
   "metadata": {},
   "source": [
    "5) Crear y Compilar el Modelo Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea1502-4dc8-434c-88ad-5defcca8d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_multimodal_v3 = keras.Model(\n",
    "    inputs=[input_tabular, input_imagenes],\n",
    "    outputs=[output_final],\n",
    "    name=\"modelo_multimodal_v3\"\n",
    ")\n",
    "\n",
    "modelo_multimodal_v3.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"✅ Modelo Multimodal V3.0 construido y compilado.\")\n",
    "modelo_multimodal_v3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94830a-a891-4b4d-a343-5d0e66afccdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Entrenamiento del Modelo Multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed476e-fd45-4ec1-b99a-049ecb4186dc",
   "metadata": {},
   "source": [
    "1) Calcular Pesos para Clases Desbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79a199-9465-441d-957a-bfd837bcdcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(f\"Pesos de clase calculados: {class_weights_dict}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e42dd-c7a8-4d3e-a44b-10f26175f18b",
   "metadata": {},
   "source": [
    "2) Callbacks (Prácticas Profesionales para el Entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa021865-cc1d-448a-b11f-3240d43b40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping: Detiene el entrenamiento si el modelo deja de mejorar.\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# ModelCheckpoint: Guarda el mejor modelo encontrado durante el entrenamiento.\n",
    "os.makedirs(\"../models/\", exist_ok=True) # Creamos la carpeta de modelos si no existe\n",
    "checkpoint_path = \"../models/multimodal_v3_best.keras\"\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss')\n",
    "\n",
    "print(f\"\\n✅ Callbacks definidos. El mejor modelo se guardará en: '{checkpoint_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b81a7-6b99-4fbf-919b-c54163fa7de2",
   "metadata": {},
   "source": [
    "3) Entrenar el Modelo Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f035991-2d06-4e5b-9918-0944d3d1d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nIniciando entrenamiento del Modelo Multimodal V3.0...\")\n",
    "\n",
    "historial_v3 = modelo_multimodal_v3.fit(\n",
    "    # Le pasamos los dos sets de datos de entrenamiento como una lista\n",
    "    [X_tabular_train_scaled, X_images_train],\n",
    "    y_train,\n",
    "    epochs=100, # Le damos un número alto de épocas, EarlyStopping decidirá cuándo parar\n",
    "    batch_size=32,\n",
    "    # Le pasamos los dos sets de datos de validación\n",
    "    validation_data=([X_tabular_test_scaled, X_images_test], y_test),\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping, model_checkpoint], # Usamos nuestros callbacks\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Entrenamiento finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
